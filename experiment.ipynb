{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c321c87b-3f21-4577-95c9-a4afbb9bcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('./common_lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3521fe18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from munch import Munch  # Munch is a dictionary that supports attribute-style access\n",
    "\n",
    "config_names = [\n",
    "    \"mini\",\n",
    "    # \"tiny\",\n",
    "    # 'small',\n",
    "    # 'medium',\n",
    "    # 'large',\n",
    "    # 'XL',\n",
    "]\n",
    "\n",
    "\n",
    "def add_exp_name(config):\n",
    "    \"\"\"Constructs the name of the log folder used to easily identify the experiment.\"\"\"\n",
    "    c = config\n",
    "    c.exp_name = \"{}_{}_{}_sl{}_h{}_ff{}_nH{}_dH{}_nl{}_seed{}{}{}\".format(\n",
    "        c.model,\n",
    "        f\"_bl{c.block_length}\",\n",
    "        c.dataset,\n",
    "        c.seq_len,\n",
    "        c.h_dim,\n",
    "        c.mlp_dim,\n",
    "        c.n_heads,\n",
    "        c.head_dim,\n",
    "        c.n_layers,\n",
    "        c.seed,\n",
    "        f\"_{c.comment}\" if c.comment else \"\",\n",
    "        \"_debug\" if c.debug else \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "## Add experiment configs\n",
    "def load_config(name=None):\n",
    "\n",
    "    c = Munch(\n",
    "        # # data\n",
    "        # data_root = \"data/books\",\n",
    "        relative_log_path=\"logs\",  # Relative path to the log folder within the project folder\n",
    "        # dataset = \"books_16384\",\n",
    "        # vocab_size = 16384,\n",
    "        debug=False,  # simply adds a \"_debug\" suffix so logs are easily distinguishable\n",
    "        # # optimiser\n",
    "        seed=41,\n",
    "        # gradient_accumulation_steps = 1,    # number of batches before doing a gradient step\n",
    "        train_batch_size=32,  # make sure batch sizes are an integer multiple of the number of workers\n",
    "        eval_batch_size=32,\n",
    "        test_batch_size=32,\n",
    "        # seq_len = 512,\n",
    "        # max_eval_steps = 512,\n",
    "        # max_train_steps = 500_000,          # total number of training steps\n",
    "        # decay_steps = 500_000,              # number of steps over which we will decay the learning rate\n",
    "        # max_lr = 0.0006,                    # starting learning rate\n",
    "        # min_lr = 0.000006,                  # final learning rate\n",
    "        # grad_clip_norm = 0.0,               # gradient norm clipping\n",
    "        # tokens_per_second = 0,              # tokens per second throughput of this config on the hardware run; used for logging over gpuhours\n",
    "        # # perform certain tasks every N steps\n",
    "        # eval_every = 1_000,                 # perform a fast evaluation (validation data)\n",
    "        # test_every = -1,                    # perform a thorough evaluation (test data)\n",
    "        # log_terminal_every = 100,           # print the current loss to terminal\n",
    "        # log_metrics_every = 100,            # log accuracy and loss metrics\n",
    "        # log_grads_every = 1_000,            # log gradients and step sizes\n",
    "        # log_activations_every = -1,         # log gradients and step sizes\n",
    "        log_ckpt_every=1_000,  # save model checkpoint to disk\n",
    "        # logging\n",
    "        comment=\"\",\n",
    "        logger_type=\"wandb\",  # can be 'tb', 'wandb' or 'all'\n",
    "        wandb_project_name=\"qlstm\",\n",
    "        dataset=\"dyck\", #[\"bit_parity\", \"dyck\", \"mqar\"]\n",
    "        model=\"lstm\", #[\"lstm\", \"qlstm\", \"gpt\"]\n",
    "        project_name=\"modern_rnns\",\n",
    "        max_steps = 40000,\n",
    "        use_flash = False\n",
    "    )\n",
    "    # default model\n",
    "    if not name or name == \"default\":\n",
    "        name = \"mini\"\n",
    "\n",
    "    # model\n",
    "    if name == \"mini\":\n",
    "        c.n_layers = 2\n",
    "        c.h_dim = 4\n",
    "        c.mlp_dim = 8\n",
    "        c.head_dim = 4\n",
    "        c.n_heads = 4\n",
    "        c.block_length = 8 # keep it equal to seq len for faster convergence\n",
    "\n",
    "        # Dataset config\n",
    "        c.output_size = 2\n",
    "        c.num_input_classes = 2\n",
    "\n",
    "        # Dyck specific\n",
    "        c.depth = 6\n",
    "        c.num_parentheses = 3\n",
    "        c.seq_len = 8\n",
    "\n",
    "        #MQAR specific\n",
    "        c.n_keys = 3\n",
    "        c.n_values = 6\n",
    "        c.train_num_pairs = \"3,3\"\n",
    "        c.eval_num_pairs = \"3,3\"\n",
    "        c.max_num_pairs = 3\n",
    "        c.unique_keys = True\n",
    "        c.all_queries_for_input = False\n",
    "\n",
    "        # Bit parity specific\n",
    "        c.train_seq_len = \"8,8\"\n",
    "        c.eval_seq_len = \"8,8\"\n",
    "        c.max_seq_len = 8\n",
    "    else:\n",
    "        raise ValueError(f\"Config name {name} is an invalid name. \")\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b807a303-6cf8-4124-ba33-6b7bb5660230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bit_parity_dataset import BitParityDatasetIterator\n",
    "from dyck_dataset import DyckDatasetIterator\n",
    "from mqar_dataset import MQARDatasetIterator\n",
    "\n",
    "def construct_dataset(config):\n",
    "    if config.dataset == \"bit_parity\":\n",
    "        train_ds = BitParityDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            sequence_length=config.train_seq_len,\n",
    "            pad_sequence_length=config.max_seq_len,\n",
    "            device=config.device,\n",
    "        )\n",
    "        eval_ds = BitParityDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            sequence_length=config.eval_seq_len,\n",
    "            pad_sequence_length=config.max_seq_len,\n",
    "            device=config.device,\n",
    "        )\n",
    "    elif config.dataset == \"dyck\":\n",
    "        train_ds = DyckDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            sequence_length=config.train_seq_len,\n",
    "            pad_sequence_length=config.max_seq_len,\n",
    "            device=config.device,\n",
    "            depth=config.depth,\n",
    "            num_parentheses=config.num_parentheses,\n",
    "        )\n",
    "        eval_ds = DyckDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            sequence_length=config.eval_seq_len,\n",
    "            pad_sequence_length=config.max_seq_len,\n",
    "            device=config.device,\n",
    "            depth=config.depth,\n",
    "            num_parentheses=config.num_parentheses,\n",
    "        )\n",
    "        config.num_input_classes = config.num_parentheses * 2 + 2\n",
    "    elif config.dataset == \"mqar\":\n",
    "        train_ds = MQARDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            num_pairs=config.train_num_pairs,\n",
    "            n_keys=config.n_keys,\n",
    "            n_values=config.n_values,\n",
    "            pad_num_pairs=config.max_num_pairs,\n",
    "            unique_keys=config.unique_keys,\n",
    "            all_queries_for_input=config.all_queries_for_input,\n",
    "            device=config.device,\n",
    "        )\n",
    "        eval_ds = MQARDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            num_pairs=config.eval_num_pairs,\n",
    "            n_keys=config.n_keys,\n",
    "            n_values=config.n_values,\n",
    "            pad_num_pairs=config.max_num_pairs,\n",
    "            unique_keys=config.unique_keys,\n",
    "            all_queries_for_input=config.all_queries_for_input,\n",
    "            device=config.device,\n",
    "        )\n",
    "        config.num_input_classes = max(config.n_keys, config.n_values + 1) + 1\n",
    "        config.output_size = config.n_values + 1\n",
    "        config.max_seq_len = max(config.max_num_pairs * 3, config.max_seq_len)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset {config.dataset} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "        \n",
    "    return train_ds, eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33dd22a-e250-4d5b-9c95-bd88d8ea0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.bit_parity_trainer import BitParityTrainer\n",
    "from trainers.dyck_trainer import DyckTrainer\n",
    "from trainers.mqar_trainer import MQARTrainer\n",
    "\n",
    "def construct_trainerClass(config):\n",
    "    if config.dataset == \"bit_parity\":\n",
    "        trainer = BitParityTrainer\n",
    "    elif config.dataset == \"dyck\":\n",
    "        trainer = DyckTrainer\n",
    "    elif config.dataset == \"mqar\":\n",
    "        trainer.dataset = MQARTrainer\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset {config.dataset} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80586e71-380c-417d-861d-d0a321376f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projects.gpt.modelgpt import ModelGPT\n",
    "from projects.lstm.basic_lstm_model import ModelLSTM\n",
    "from projects.qlstm.modelqlstm import ModelQLSTM\n",
    "\n",
    "def construct_model(config):\n",
    "    if config.model == \"gpt\":\n",
    "        model = ModelGPT(config = config)\n",
    "    elif config.model == \"lstm\":\n",
    "        model = ModelLSTM(config = config)\n",
    "    elif config.model == \"qlstm\":\n",
    "        model = ModelQLSTM(config = config)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Model {config.model} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc81df0-9786-4ab6-bf8e-f9fd03dc3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def set_up_seeds(config):\n",
    "    torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5d10fd-df72-4dad-b1ee-09d695583b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    \n",
    "    set_up_seeds(config)\n",
    "    train_ds, eval_ds = construct_dataset(config)\n",
    "    trainerClass = construct_trainerClass(config)\n",
    "    model = construct_model(config).to(config.device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.95), eps=1e-08)\n",
    "    logger = experiment_utils.setup_experiment(config)\n",
    "    \n",
    "    trainer = trainerClass(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        train_loader=train_ds,\n",
    "        eval_loader=eval_ds,\n",
    "        optimizer=opt,\n",
    "        device=config.device,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c294b550-4913-4b78-b714-8edd3ca247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wrapper(config):\n",
    "    import sys, os\n",
    "    sys.path.append('./common_lib')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        config.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        mprint(\"Cuda is not available, using CPU instead.\")\n",
    "        config.device = torch.device(\"cpu\")\n",
    "    config.project_path = os.path.join(os.getcwd(), 'projects', config.model)\n",
    "    add_exp_name(config)\n",
    "    print(config.dataset, config.model, config.train_seq_len, config.eval_seq_len)\n",
    "    config.run_name = f\"{config.model}_{config.dataset}_par_{config.num_parentheses}_depth_{config.depth}_{config.train_seq_len}_{config.eval_seq_len}\"\n",
    "    \n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "    wandb.init(project=config.project_name, entity=\"www-vickyzeu\", config=config, name = config.run_name)\n",
    "    return run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c08d31d-3292-466d-93fa-bfa600980529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "import wandb\n",
    "from common_lib import experiment_utils\n",
    "\n",
    "train_seq_lens = [f\"{x},{x}\" for x in [2 ** i for i in range(4, 9)]]\n",
    "eval_seq_lens = [f\"{x},{x}\" for x in [2 ** i + 2 ** (i - 1) for i in range(4, 9)]]\n",
    "models = [\"lstm\", \"gpt\", \"qlstm\"]\n",
    "\n",
    "def train(local=False):\n",
    "    config = load_config()\n",
    "    print('key', os.getenv(\"WANDB_API_KEY\"))\n",
    "    if local:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "        run(config)\n",
    "    else:\n",
    "        executor = submitit.AutoExecutor(folder=\"logs/slurm\")\n",
    "        executor.update_parameters(\n",
    "            timeout_min=60*4,\n",
    "            tasks_per_node=1,\n",
    "            #cpus_per_task=4,\n",
    "            account=\"pmlr\",\n",
    "            name=\"pmlr_training\"\n",
    "        )\n",
    "        jobs = []\n",
    "        for train_seq_len, eval_seq_len in zip(train_seq_lens[1:4], eval_seq_lens[1:4]):\n",
    "            for model in models[:1]:\n",
    "                config.train_seq_len = train_seq_len\n",
    "                config.eval_seq_len = eval_seq_len\n",
    "                config.max_seq_len = max(int(train_seq_len.split(',')[-1]), int(eval_seq_len.split(',')[-1]))\n",
    "                config.model = model\n",
    "                config.block_length = min(8, config.max_seq_len)\n",
    "                jobs.append(executor.submit(run_wrapper, config))\n",
    "                \n",
    "        print(f\"Submitted {len(jobs)} jobs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae4bb5b-c1d2-4e79-963a-072c9aafa27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 3585267dd9c6986318c56af0efac3ab16956dabc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzarzu/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/auto/auto.py:23: UserWarning: Setting 'account' is deprecated. Use 'slurm_account' instead.\n",
      "  warnings.warn(f\"Setting '{arg}' is deprecated. Use '{new_arg}' instead.\")\n",
      "sbatch: error: GPU count: 1\n",
      "sbatch: error: CPU count: 2\n",
      "sbatch: error: Memory: 24576MB\n",
      "sbatch: error: QOSMaxSubmitJobPerUserLimit\n",
      "sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n"
     ]
    },
    {
     "ename": "FailedJobError",
     "evalue": "sbatch: error: GPU count: 1\nsbatch: error: CPU count: 2\nsbatch: error: Memory: 24576MB\nsbatch: error: QOSMaxSubmitJobPerUserLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['sbatch', '/home/vzarzu/modern_rnns/logs/slurm/.submission_file_8f11594ae03d4b52840ee14c4679ea9c.sh']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedJobError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# won't work in Jupyter\u001b[39;00m\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(local)\u001b[0m\n\u001b[1;32m     33\u001b[0m         config\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     34\u001b[0m         config\u001b[38;5;241m.\u001b[39mblock_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m8\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_seq_len)\n\u001b[0;32m---> 35\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmitted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(jobs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m jobs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/core/core.py:734\u001b[0m, in \u001b[0;36mExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delayed_batch\u001b[38;5;241m.\u001b[39mappend((job, ds))\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_process_submissions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(job) \u001b[38;5;129;01mis\u001b[39;00m Job:  \u001b[38;5;66;03m# pylint: disable=unidiomatic-typecheck\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecutors should never return a base Job class (implementation issue)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/auto/auto.py:218\u001b[0m, in \u001b[0;36mAutoExecutor._internal_process_submissions\u001b[0;34m(self, delayed_submissions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_internal_process_submissions\u001b[39m(\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m, delayed_submissions: tp\u001b[38;5;241m.\u001b[39mList[DelayedSubmission]\n\u001b[1;32m    217\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mList[Job[tp\u001b[38;5;241m.\u001b[39mAny]]:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_process_submissions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_submissions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/slurm/slurm.py:317\u001b[0m, in \u001b[0;36mSlurmExecutor._internal_process_submissions\u001b[0;34m(self, delayed_submissions)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_internal_process_submissions\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m, delayed_submissions: tp\u001b[38;5;241m.\u001b[39mList[utils\u001b[38;5;241m.\u001b[39mDelayedSubmission]\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mList[core\u001b[38;5;241m.\u001b[39mJob[tp\u001b[38;5;241m.\u001b[39mAny]]:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(delayed_submissions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_process_submissions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_submissions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# array\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     folder \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mJobPaths\u001b[38;5;241m.\u001b[39mget_first_id_independent_folder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolder)\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/core/core.py:893\u001b[0m, in \u001b[0;36mPicklingExecutor._internal_process_submissions\u001b[0;34m(self, delayed_submissions)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_throttle()\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_job_submitted \u001b[38;5;241m=\u001b[39m _time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 893\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submitit_command_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m job\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mmove_temporary_file(pickle_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmitted_pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    895\u001b[0m jobs\u001b[38;5;241m.\u001b[39mappend(job)\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/core/core.py:934\u001b[0m, in \u001b[0;36mPicklingExecutor._submit_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    932\u001b[0m command_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_submission_command(submission_file_path)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCommandFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# explicit errors\u001b[39;00m\n\u001b[1;32m    935\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_job_id_from_submission_command(output)\n\u001b[1;32m    936\u001b[0m tasks_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_tasks()))\n",
      "File \u001b[0;32m~/miniconda3/envs/cil/lib/python3.10/site-packages/submitit/core/utils.py:358\u001b[0m, in \u001b[0;36mCommandFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[1;32m    355\u001b[0m         subprocess_error \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(\n\u001b[1;32m    356\u001b[0m             retcode, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr\n\u001b[1;32m    357\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FailedJobError(stderr) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess_error\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stdout\n",
      "\u001b[0;31mFailedJobError\u001b[0m: sbatch: error: GPU count: 1\nsbatch: error: CPU count: 2\nsbatch: error: Memory: 24576MB\nsbatch: error: QOSMaxSubmitJobPerUserLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env_path = os.path.join('.', '.env')  # won't work in Jupyter\n",
    "load_dotenv()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87c21b-380f-4d1b-895a-ff9721e572bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
