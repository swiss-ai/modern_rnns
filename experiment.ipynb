{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321c87b-3f21-4577-95c9-a4afbb9bcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "sys.path.append('./common_lib')\n",
    "\n",
    "def set_up_seeds(config):\n",
    "    torch.manual_seed(config.seed)\n",
    "\n",
    "WANDB_ENTITY = \"www-vickyzeu\"\n",
    "\n",
    "# Set which sequence lengths and models you want to run here.\n",
    "\n",
    "# These were our experiments evaluating on much longer sequences than training.\n",
    "train_seq_lens = [\"32,32\", \"32,32\", \"32,32\"]\n",
    "eval_seq_lens = [\"64,64\", \"128,128\", \"256,256\"]\n",
    "\n",
    "# There were our experiments evaluating on 50% longer sequence lengths as training.\n",
    "# train_seq_lens = [f\"{x},{x}\" for x in [2 ** i for i in range(5, 9)]]\n",
    "# eval_seq_lens = [f\"{x},{x}\" for x in [2 ** i + 2 ** (i - 1) for i in range(5, 9)]]\n",
    "\n",
    "models = [\"gpt\", \"lstm\", \"qlstm\", \"lin_transformer\", \"lru\", \"delta_net\", \"mamba\"]\n",
    "\n",
    "# Set which dataset to run\n",
    "DATASET = \"bit_parity\" # \"bit_parity\", \"dyck\", \"mqar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import Munch  # Munch is a dictionary that supports attribute-style access\n",
    "\n",
    "config_names = [\n",
    "    \"mini\",\n",
    "]\n",
    "\n",
    "\n",
    "def add_exp_name(config):\n",
    "    \"\"\"Constructs the name of the log folder used to easily identify the experiment.\"\"\"\n",
    "    c = config\n",
    "    c.exp_name = \"{}_{}_{}_sl{}_h{}_ff{}_nH{}_dH{}_nl{}_seed{}{}{}\".format(\n",
    "        c.model,\n",
    "        f\"_bl{c.block_length}\",\n",
    "        c.dataset,\n",
    "        c.seq_len,\n",
    "        c.h_dim,\n",
    "        c.mlp_dim,\n",
    "        c.n_heads,\n",
    "        c.head_dim,\n",
    "        c.n_layers,\n",
    "        c.seed,\n",
    "        f\"_{c.comment}\" if c.comment else \"\",\n",
    "        \"_debug\" if c.debug else \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "## Add experiment configs\n",
    "def load_config(name=None):\n",
    "\n",
    "    c = Munch(\n",
    "        # # data\n",
    "        relative_log_path=\"logs\",  # Relative path to the log folder within the project folder\n",
    "        debug=False,  # simply adds a \"_debug\" suffix so logs are easily distinguishable\n",
    "        # # optimiser\n",
    "        seed=41,\n",
    "        # gradient_accumulation_steps = 1,    # number of batches before doing a gradient step\n",
    "        train_batch_size=32,  # make sure batch sizes are an integer multiple of the number of workers\n",
    "        eval_batch_size=32,\n",
    "        test_batch_size=32,\n",
    "        # seq_len = 512,\n",
    "        # max_eval_steps = 512,\n",
    "        # max_train_steps = 500_000,          # total number of training steps\n",
    "        # decay_steps = 500_000,              # number of steps over which we will decay the learning rate\n",
    "        # max_lr = 0.0006,                    # starting learning rate\n",
    "        # min_lr = 0.000006,                  # final learning rate\n",
    "        # grad_clip_norm = 0.0,               # gradient norm clipping\n",
    "        # tokens_per_second = 0,              # tokens per second throughput of this config on the hardware run; used for logging over gpuhours\n",
    "        # # perform certain tasks every N steps\n",
    "        # eval_every = 1_000,                 # perform a fast evaluation (validation data)\n",
    "        # test_every = -1,                    # perform a thorough evaluation (test data)\n",
    "        # log_terminal_every = 100,           # print the current loss to terminal\n",
    "        # log_metrics_every = 100,            # log accuracy and loss metrics\n",
    "        # log_grads_every = 1_000,            # log gradients and step sizes\n",
    "        # log_activations_every = -1,         # log gradients and step sizes\n",
    "        log_ckpt_every=1_000,  # save model checkpoint to disk\n",
    "        # logging\n",
    "        comment=\"\",\n",
    "        logger_type=\"wandb\",  # can be 'tb', 'wandb' or 'all'\n",
    "        wandb_project_name=\"associative_rnns\",\n",
    "        dataset=DATASET, #[\"bit_parity\", \"dyck\", \"mqar\"]\n",
    "        project_name=\"associative_rnns\",\n",
    "        project_path=\"./projects/lstm/\", # Hack, just used to save the source code\n",
    "        exp_name=\"\",\n",
    "        max_steps = 40000,\n",
    "        use_flash = False,\n",
    "        device = \"cuda\"\n",
    "    )\n",
    "    # default model\n",
    "    if not name or name == \"default\":\n",
    "        name = \"mini\"\n",
    "\n",
    "    # model\n",
    "    if name == \"mini\":\n",
    "        c.n_layers = 2\n",
    "        c.h_dim = 4\n",
    "        c.mlp_dim = 8\n",
    "        c.head_dim = 4\n",
    "        c.n_heads = 4\n",
    "        c.block_length = 8 # keep it equal to seq len for faster convergence\n",
    "\n",
    "        # Mamba\n",
    "        c.d_model = 128\n",
    "        c.d_state = 8\n",
    "        c.d_conv = 3\n",
    "        c.expand = 2\n",
    "        c.dt_rank = 1\n",
    "\n",
    "        # Dataset config\n",
    "        c.output_size = 2\n",
    "        c.num_input_classes = 2\n",
    "\n",
    "        # Dyck specific\n",
    "        c.depth = 6\n",
    "        c.num_parentheses = 3\n",
    "        c.seq_len = 8\n",
    "\n",
    "        #MQAR specific\n",
    "        c.n_keys = 3\n",
    "        c.n_values = 6\n",
    "        c.train_num_pairs = \"3,3\"\n",
    "        c.eval_num_pairs = \"3,3\"\n",
    "        c.max_num_pairs = 3\n",
    "        c.unique_keys = True\n",
    "        c.all_queries_for_input = False\n",
    "\n",
    "        # Bit parity specific\n",
    "        c.train_seq_len = \"8,8\"\n",
    "        c.eval_seq_len = \"8,8\"\n",
    "        c.num_ones = 12 # Fixed number of 1s in the training sequences. Use None to disable this\n",
    "        c.max_seq_len = 8\n",
    "    else:\n",
    "        raise ValueError(f\"Config name {name} is an invalid name. \")\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807a303-6cf8-4124-ba33-6b7bb5660230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.bit_parity_dataset import BitParityDatasetIterator\n",
    "from datasets.dyck_dataset import DyckDatasetIterator\n",
    "from datasets.mqar_dataset import MQARDatasetIterator\n",
    "\n",
    "def construct_dataset(config):\n",
    "    if config.dataset == \"bit_parity\":\n",
    "        train_ds = BitParityDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            sequence_length=config.train_seq_len,\n",
    "            num_ones=config.num_ones,\n",
    "            device=config.device,\n",
    "        )\n",
    "        eval_ds = BitParityDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            sequence_length=config.eval_seq_len,\n",
    "            num_ones=config.num_ones,\n",
    "            device=config.device,\n",
    "        )\n",
    "    elif config.dataset == \"dyck\":\n",
    "        train_ds = DyckDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            sequence_length=config.train_seq_len,\n",
    "            device=config.device,\n",
    "            depth=config.depth,\n",
    "            num_parentheses=config.num_parentheses,\n",
    "        )\n",
    "        eval_ds = DyckDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            sequence_length=config.eval_seq_len,\n",
    "            device=config.device,\n",
    "            depth=config.depth,\n",
    "            num_parentheses=config.num_parentheses,\n",
    "        )\n",
    "        config.num_input_classes = config.num_parentheses * 2 + 2\n",
    "    elif config.dataset == \"mqar\":\n",
    "        train_ds = MQARDatasetIterator(\n",
    "            batch_size=config.train_batch_size,\n",
    "            num_pairs=config.train_num_pairs,\n",
    "            n_keys=config.n_keys,\n",
    "            n_values=config.n_values,\n",
    "            unique_keys=config.unique_keys,\n",
    "            all_queries_for_input=config.all_queries_for_input,\n",
    "            device=config.device,\n",
    "        )\n",
    "        eval_ds = MQARDatasetIterator(\n",
    "            batch_size=config.eval_batch_size,\n",
    "            num_pairs=config.eval_num_pairs,\n",
    "            n_keys=config.n_keys,\n",
    "            n_values=config.n_values,\n",
    "            unique_keys=config.unique_keys,\n",
    "            all_queries_for_input=config.all_queries_for_input,\n",
    "            device=config.device,\n",
    "        )\n",
    "        config.num_input_classes = max(config.n_keys, config.n_values + 1) + 1\n",
    "        config.output_size = config.n_values + 1\n",
    "        config.max_seq_len = max(config.max_num_pairs * 3, config.max_seq_len)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset {config.dataset} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "\n",
    "    return train_ds, eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dd22a-e250-4d5b-9c95-bd88d8ea0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.bit_parity_trainer import BitParityTrainer\n",
    "from trainers.dyck_trainer import DyckTrainer\n",
    "from trainers.mqar_trainer import MQARTrainer\n",
    "\n",
    "def construct_trainerClass(config):\n",
    "    if config.dataset == \"bit_parity\":\n",
    "        trainer = BitParityTrainer\n",
    "    elif config.dataset == \"dyck\":\n",
    "        trainer = DyckTrainer\n",
    "    elif config.dataset == \"mqar\":\n",
    "        trainer = MQARTrainer\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset {config.dataset} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80586e71-380c-417d-861d-d0a321376f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projects.gpt.modelgpt import ModelGPT\n",
    "from projects.lstm.modellstm import ModelLSTM\n",
    "from projects.qlstm.modelqlstm import ModelQLSTM\n",
    "from projects.linearTransformer.modelLinTransformer import ModelLinTransformer\n",
    "from projects.lru.modellru import ModelLRU\n",
    "from projects.deltanet.modelDeltanet import ModelDeltaNet\n",
    "from projects.mamba.modelmamba import ModelMamba\n",
    "\n",
    "def construct_model(config):\n",
    "    if config.model == \"gpt\":\n",
    "        model = ModelGPT(config = config)\n",
    "    elif config.model == \"lstm\":\n",
    "        model = ModelLSTM(config = config)\n",
    "    elif config.model == \"qlstm\":\n",
    "        model = ModelQLSTM(config = config)\n",
    "    elif config.model == \"lin_transformer\":\n",
    "        model = ModelLinTransformer(config = config)\n",
    "    elif config.model == \"lru\":\n",
    "        model = ModelLRU(config = config)\n",
    "    elif config.model == \"delta_net\":\n",
    "        model = ModelDeltaNet(config = config)\n",
    "    elif config.model == \"mamba\":\n",
    "        model = ModelMamba(config = config)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Model {config.model} not supported. Please add the configuration for this dataset.\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d10fd-df72-4dad-b1ee-09d695583b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "\n",
    "    set_up_seeds(config)\n",
    "    train_ds, eval_ds = construct_dataset(config)\n",
    "    trainerClass = construct_trainerClass(config)\n",
    "    model = construct_model(config).to(config.device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.95), eps=1e-08)\n",
    "    logger = experiment_utils.setup_experiment(config)\n",
    "\n",
    "    trainer = trainerClass(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        train_loader=train_ds,\n",
    "        eval_loader=eval_ds,\n",
    "        optimizer=opt,\n",
    "        device=config.device,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_run_name(config):\n",
    "    if config.dataset == \"dyck\":\n",
    "        return f\"{config.model}_{config.dataset}_par_{config.num_parentheses}_depth_{config.depth}_{config.train_seq_len}_{config.eval_seq_len}\"\n",
    "    elif config.dataset == \"bit_parity\":\n",
    "        ones_string = f\"_ones_{config.num_ones}\" if config.num_ones else \"\"\n",
    "        return f\"{config.model}_{config.dataset}{ones_string}_{config.train_seq_len}_{config.eval_seq_len}\"\n",
    "    elif config.dataset == \"mqar\":\n",
    "        return f\"{config.model}_{config.dataset}_keys_{config.n_keys}_values_{config.n_values}_pairs_{config.train_num_pairs}_{config.eval_num_pairs}\"\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"No run name pattern found for {config.dataset}, please add one.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294b550-4913-4b78-b714-8edd3ca247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wrapper(config):\n",
    "    import sys, os\n",
    "    sys.path.append('./common_lib')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        config.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        mprint(\"Cuda is not available, using CPU instead.\")\n",
    "        config.device = torch.device(\"cpu\")\n",
    "    config.project_path = os.path.join(os.getcwd(), 'projects', config.model)\n",
    "    add_exp_name(config)\n",
    "    print(config.dataset, config.model, config.train_seq_len, config.eval_seq_len)\n",
    "    config.run_name = get_config_run_name(config)\n",
    "\n",
    "\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "    wandb.init(project=config.project_name, entity=WANDB_ENTITY, config=config, name = config.run_name)\n",
    "    return run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08d31d-3292-466d-93fa-bfa600980529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "import wandb\n",
    "from common_lib import experiment_utils\n",
    "\n",
    "def train(local=False):\n",
    "    config = load_config()\n",
    "    if local:\n",
    "\n",
    "        for train_seq_len, eval_seq_len in zip(train_seq_lens, eval_seq_lens):\n",
    "            for model in models:\n",
    "                config.train_seq_len = train_seq_len\n",
    "                config.eval_seq_len = eval_seq_len\n",
    "                config.max_seq_len = max(int(train_seq_len.split(',')[-1]), int(eval_seq_len.split(',')[-1]))\n",
    "                config.model = model\n",
    "                config.block_length = min(8, config.max_seq_len)\n",
    "                config.exp_name = f\"{model}_sl{train_seq_len}_esl{eval_seq_len}_bl{config.block_length}\"\n",
    "                config.run_name = get_config_run_name(config)\n",
    "\n",
    "                from dotenv import load_dotenv\n",
    "                load_dotenv()\n",
    "                wandb.init(project=config.project_name, entity=WANDB_ENTITY, config=config, name = config.run_name)\n",
    "                run(config)\n",
    "                wandb.finish()\n",
    "    else:\n",
    "        executor = submitit.AutoExecutor(folder=\"logs/slurm\")\n",
    "        executor.update_parameters(\n",
    "            timeout_min=60*6,\n",
    "            tasks_per_node=1,\n",
    "            #cpus_per_task=4,\n",
    "            account=\"pmlr_jobs\",\n",
    "            name=\"pmlr_training\"\n",
    "        )\n",
    "        jobs = []\n",
    "        for train_seq_len, eval_seq_len in zip(train_seq_lens, eval_seq_lens):\n",
    "            for model in models:\n",
    "                config.train_seq_len = train_seq_len\n",
    "                config.eval_seq_len = eval_seq_len\n",
    "                config.max_seq_len = max(int(train_seq_len.split(',')[-1]), int(eval_seq_len.split(',')[-1]))\n",
    "                config.model = model\n",
    "                config.block_length = min(8, config.max_seq_len)\n",
    "                jobs.append(executor.submit(run_wrapper, config))\n",
    "\n",
    "        print(f\"Submitted {len(jobs)} jobs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4bb5b-c1d2-4e79-963a-072c9aafa27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env_path = os.path.join('.', '.env')  # won't work in Jupyter\n",
    "load_dotenv()\n",
    "train(local=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
